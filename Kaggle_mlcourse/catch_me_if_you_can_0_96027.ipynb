{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import TimeSeriesSplit, cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder, Normalizer, MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from matplotlib import pyplot as plt\n",
    "#import eli5\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_submission_file(predicted_labels, out_file,\n",
    "                             target='target', index_label=\"session_id\"):\n",
    "    predicted_df = pd.DataFrame(predicted_labels,\n",
    "                                index = np.arange(1, predicted_labels.shape[0] + 1),\n",
    "                                columns=[target])\n",
    "    predicted_df.to_csv(out_file, index_label=index_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_confusion_matrix(y_true,y_pred,title='Confusionmatrix'):\n",
    "    table=confusion_matrix(y_true,y_pred)\n",
    "    fig,ax=plt.subplots(frameon=False)\n",
    "    fig.set_size_inches(4,3)\n",
    "    fig.suptitle(title,fontsize=20)\n",
    "    ax.axis('off')\n",
    "    ax.xaxis.set_visible(False)\n",
    "    ax.yaxis.set_visible(False)\n",
    "\n",
    "    the_table=ax.table(cellText=table,\n",
    "                        colWidths=[0.5]*len([0,1]),\n",
    "                        rowLabels=['True 0','True 1'],colLabels=['Predicted 0','Predicted 1'],\n",
    "                        cellLoc='center',rowLoc='center',loc=\"center\")\n",
    "    the_table.set_fontsize(34)\n",
    "    the_table.scale(1,4)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../input/catch-me-if-you-can-intruder-detection-through-webpage-session-tracking2/site_dic.pkl', 'rb') as input_file:\n",
    "    site_dict = pickle.load(input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>site1</th>\n",
       "      <th>time1</th>\n",
       "      <th>site2</th>\n",
       "      <th>time2</th>\n",
       "      <th>site3</th>\n",
       "      <th>time3</th>\n",
       "      <th>site4</th>\n",
       "      <th>time4</th>\n",
       "      <th>site5</th>\n",
       "      <th>time5</th>\n",
       "      <th>...</th>\n",
       "      <th>time6</th>\n",
       "      <th>site7</th>\n",
       "      <th>time7</th>\n",
       "      <th>site8</th>\n",
       "      <th>time8</th>\n",
       "      <th>site9</th>\n",
       "      <th>time9</th>\n",
       "      <th>site10</th>\n",
       "      <th>time10</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>session_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>21669</td>\n",
       "      <td>56</td>\n",
       "      <td>2013-01-12 08:05:57</td>\n",
       "      <td>55.0</td>\n",
       "      <td>2013-01-12 08:05:57</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>...</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54843</td>\n",
       "      <td>56</td>\n",
       "      <td>2013-01-12 08:37:23</td>\n",
       "      <td>55.0</td>\n",
       "      <td>2013-01-12 08:37:23</td>\n",
       "      <td>56.0</td>\n",
       "      <td>2013-01-12 09:07:07</td>\n",
       "      <td>55.0</td>\n",
       "      <td>2013-01-12 09:07:09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>...</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77292</td>\n",
       "      <td>946</td>\n",
       "      <td>2013-01-12 08:50:13</td>\n",
       "      <td>946.0</td>\n",
       "      <td>2013-01-12 08:50:14</td>\n",
       "      <td>951.0</td>\n",
       "      <td>2013-01-12 08:50:15</td>\n",
       "      <td>946.0</td>\n",
       "      <td>2013-01-12 08:50:15</td>\n",
       "      <td>946.0</td>\n",
       "      <td>2013-01-12 08:50:16</td>\n",
       "      <td>...</td>\n",
       "      <td>2013-01-12 08:50:16</td>\n",
       "      <td>948.0</td>\n",
       "      <td>2013-01-12 08:50:16</td>\n",
       "      <td>784.0</td>\n",
       "      <td>2013-01-12 08:50:16</td>\n",
       "      <td>949.0</td>\n",
       "      <td>2013-01-12 08:50:17</td>\n",
       "      <td>946.0</td>\n",
       "      <td>2013-01-12 08:50:17</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>114021</td>\n",
       "      <td>945</td>\n",
       "      <td>2013-01-12 08:50:17</td>\n",
       "      <td>948.0</td>\n",
       "      <td>2013-01-12 08:50:17</td>\n",
       "      <td>949.0</td>\n",
       "      <td>2013-01-12 08:50:18</td>\n",
       "      <td>948.0</td>\n",
       "      <td>2013-01-12 08:50:18</td>\n",
       "      <td>945.0</td>\n",
       "      <td>2013-01-12 08:50:18</td>\n",
       "      <td>...</td>\n",
       "      <td>2013-01-12 08:50:18</td>\n",
       "      <td>947.0</td>\n",
       "      <td>2013-01-12 08:50:19</td>\n",
       "      <td>945.0</td>\n",
       "      <td>2013-01-12 08:50:19</td>\n",
       "      <td>946.0</td>\n",
       "      <td>2013-01-12 08:50:19</td>\n",
       "      <td>946.0</td>\n",
       "      <td>2013-01-12 08:50:20</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>146670</td>\n",
       "      <td>947</td>\n",
       "      <td>2013-01-12 08:50:20</td>\n",
       "      <td>950.0</td>\n",
       "      <td>2013-01-12 08:50:20</td>\n",
       "      <td>948.0</td>\n",
       "      <td>2013-01-12 08:50:20</td>\n",
       "      <td>947.0</td>\n",
       "      <td>2013-01-12 08:50:21</td>\n",
       "      <td>950.0</td>\n",
       "      <td>2013-01-12 08:50:21</td>\n",
       "      <td>...</td>\n",
       "      <td>2013-01-12 08:50:21</td>\n",
       "      <td>946.0</td>\n",
       "      <td>2013-01-12 08:50:21</td>\n",
       "      <td>951.0</td>\n",
       "      <td>2013-01-12 08:50:22</td>\n",
       "      <td>946.0</td>\n",
       "      <td>2013-01-12 08:50:22</td>\n",
       "      <td>947.0</td>\n",
       "      <td>2013-01-12 08:50:22</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            site1               time1  site2               time2  site3  \\\n",
       "session_id                                                                \n",
       "21669          56 2013-01-12 08:05:57   55.0 2013-01-12 08:05:57    NaN   \n",
       "54843          56 2013-01-12 08:37:23   55.0 2013-01-12 08:37:23   56.0   \n",
       "77292         946 2013-01-12 08:50:13  946.0 2013-01-12 08:50:14  951.0   \n",
       "114021        945 2013-01-12 08:50:17  948.0 2013-01-12 08:50:17  949.0   \n",
       "146670        947 2013-01-12 08:50:20  950.0 2013-01-12 08:50:20  948.0   \n",
       "\n",
       "                         time3  site4               time4  site5  \\\n",
       "session_id                                                         \n",
       "21669                      NaT    NaN                 NaT    NaN   \n",
       "54843      2013-01-12 09:07:07   55.0 2013-01-12 09:07:09    NaN   \n",
       "77292      2013-01-12 08:50:15  946.0 2013-01-12 08:50:15  946.0   \n",
       "114021     2013-01-12 08:50:18  948.0 2013-01-12 08:50:18  945.0   \n",
       "146670     2013-01-12 08:50:20  947.0 2013-01-12 08:50:21  950.0   \n",
       "\n",
       "                         time5  ...               time6  site7  \\\n",
       "session_id                      ...                              \n",
       "21669                      NaT  ...                 NaT    NaN   \n",
       "54843                      NaT  ...                 NaT    NaN   \n",
       "77292      2013-01-12 08:50:16  ... 2013-01-12 08:50:16  948.0   \n",
       "114021     2013-01-12 08:50:18  ... 2013-01-12 08:50:18  947.0   \n",
       "146670     2013-01-12 08:50:21  ... 2013-01-12 08:50:21  946.0   \n",
       "\n",
       "                         time7  site8               time8  site9  \\\n",
       "session_id                                                         \n",
       "21669                      NaT    NaN                 NaT    NaN   \n",
       "54843                      NaT    NaN                 NaT    NaN   \n",
       "77292      2013-01-12 08:50:16  784.0 2013-01-12 08:50:16  949.0   \n",
       "114021     2013-01-12 08:50:19  945.0 2013-01-12 08:50:19  946.0   \n",
       "146670     2013-01-12 08:50:21  951.0 2013-01-12 08:50:22  946.0   \n",
       "\n",
       "                         time9 site10              time10 target  \n",
       "session_id                                                        \n",
       "21669                      NaT    NaN                 NaT      0  \n",
       "54843                      NaT    NaN                 NaT      0  \n",
       "77292      2013-01-12 08:50:17  946.0 2013-01-12 08:50:17      0  \n",
       "114021     2013-01-12 08:50:19  946.0 2013-01-12 08:50:20      0  \n",
       "146670     2013-01-12 08:50:22  947.0 2013-01-12 08:50:22      0  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "times = ['time%s' % i for i in range(1, 11)]\n",
    "train_df = pd.read_csv('../input/catch-me-if-you-can-intruder-detection-through-webpage-session-tracking2/train_sessions.csv',\n",
    "                       index_col='session_id', parse_dates=times)\n",
    "test_df = pd.read_csv('../input/catch-me-if-you-can-intruder-detection-through-webpage-session-tracking2/test_sessions.csv',\n",
    "                      index_col='session_id', parse_dates=times)\n",
    "\n",
    "# Sort the data by time\n",
    "train_df = train_df.sort_values(by='time1')\n",
    "\n",
    "\n",
    "# Look at the first rows of the training set\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sites = ['site%s' % i for i in range(1, 11)]\n",
    "train_df[sites].fillna(0).astype('int').to_csv('train_sessions_text.txt', \n",
    "                                               sep=' ', \n",
    "                       index=None, header=None)\n",
    "test_df[sites].fillna(0).astype('int').to_csv('test_sessions_text.txt', \n",
    "                                              sep=' ', \n",
    "                       index=None, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56 55 0 0 0 0 0 0 0 0\r\n",
      "56 55 56 55 0 0 0 0 0 0\r\n",
      "946 946 951 946 946 945 948 784 949 946\r\n",
      "945 948 949 948 945 946 947 945 946 946\r\n",
      "947 950 948 947 950 952 946 951 946 947\r\n"
     ]
    }
   ],
   "source": [
    "!head -5 train_sessions_text.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((253561, 25000), (82797, 25000))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#cv = CountVectorizer(ngram_range=(1, 3), max_features=50000)\n",
    "#cv = TfidfVectorizer(ngram_range=(1, 3), max_df=0.9)\n",
    "cv = TfidfVectorizer(ngram_range=(1, 3), max_features=25000, binary=True, sublinear_tf=True)\n",
    "\n",
    "with open('train_sessions_text.txt') as inp_train_file:\n",
    "    X_train = cv.fit_transform(inp_train_file)\n",
    "with open('test_sessions_text.txt') as inp_test_file:\n",
    "    X_test = cv.transform(inp_test_file)\n",
    "\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0001294681916051883, 0.0, 1.0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.mean(), X_train.min(), X_train.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train_df['target'].astype('int').values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_split = TimeSeriesSplit(n_splits=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit2 = LogisticRegression(C=1, random_state=17, solver='liblinear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cv_scores = cross_val_score(logit2, X_train, y_train, cv=time_split, \n",
    "                            #scoring='roc_auc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cv_scores, cv_scores.mean() # 0.869185314739268"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_alice_day_of_week(x):\n",
    "    x = x.weekday()\n",
    "    if x == 0 or x == 1 or x == 3 or x == 4:\n",
    "      return 1\n",
    "    return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_time_features(df, X_sparse):\n",
    "    hour = df['time1'].apply(lambda ts: ts.hour)\n",
    "    morning = ((hour >= 7) & (hour <= 11)).astype('int')\n",
    "    day = ((hour >= 12) & (hour <= 18)).astype('int')\n",
    "    evening = ((hour >= 19) & (hour <= 23)).astype('int')\n",
    "    night = ((hour >= 0) & (hour <= 6)).astype('int')\n",
    "    aalice_hour = ((hour >= 16) & (hour <= 17)).astype('int')\n",
    "    \n",
    "    # added in 27\n",
    "    day_of_week_all = df['time1'].apply(lambda t: t.weekday()).values.reshape(-1, 1)\n",
    "    day_of_week = df['time1'].apply(lambda x: is_alice_day_of_week(x)).values.reshape(-1, 1)\n",
    "    #foo['verisign_ids start8'] = df['site8'].apply(lambda x: is_site(x, verisign_ids))\n",
    "    \n",
    "    month = df['time1'].apply(lambda t: t.month).values.reshape(-1, 1)\n",
    "    #year_month = times['time1'].apply(lambda t: 100 * t.year + t.month).values.reshape(-1, 1) / 1e5\n",
    "    \n",
    "    # aalice_hour and most common day\n",
    "    \n",
    "    X = hstack([X_sparse,\n",
    "                morning.values.reshape(-1, 1), \n",
    "                day.values.reshape(-1, 1),\n",
    "                evening.values.reshape(-1, 1),\n",
    "                night.values.reshape(-1, 1),\n",
    "                #aalice_hour.values.reshape(-1, 1),\n",
    "                day_of_week,\n",
    "                #month,\n",
    "                day_of_week_all,\n",
    "                #alice_hour.reshape(-1, 1)\n",
    "                #year_month\n",
    "                ])\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_new = add_time_features(train_df, X_train)\n",
    "X_test_new = add_time_features(test_df, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((253561, 25006), (82797, 25006))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_new.shape, X_test_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_start_month_feature(df, X_sparse):\n",
    "    foo = pd.DataFrame(index=df.index)\n",
    "    bar = df['time1'].apply(lambda ts: 100 * ts.year + ts.month).astype('float64')\n",
    "    foo['scaled_month'] = StandardScaler().fit_transform(bar.values.reshape(-1, 1))\n",
    "    \n",
    "    #foo['scaled_month'] = df['time1'].apply(lambda ts: (100 * ts.year + ts.month) / 1e5).astype('float64')\n",
    "    \n",
    "    X = hstack([X_sparse, foo])\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_new2 = add_start_month_feature(train_df, X_train_new)\n",
    "X_test_new2 = add_start_month_feature(test_df, X_test_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_session_length_feature(df, X_sparse):    \n",
    "    foo = pd.DataFrame(index=df.index)\n",
    "    foo['min_seconds'] = df[times].min(axis=1)\n",
    "    foo['max_seconds'] = df[times].max(axis=1)\n",
    "    foo['seconds'] = (foo['max_seconds'] - foo['min_seconds']) / np.timedelta64(1, 's')\n",
    "\n",
    "    foo['scaled_session_duration_seconds'] = MinMaxScaler().fit_transform(foo['seconds'].values.reshape(-1, 1))\n",
    "    #foo['scaled_session_duration_seconds'] = StandardScaler().fit_transform(foo['seconds'].values.reshape(-1, 1))\n",
    "    \n",
    "    #foo['month'] = df['time1'].apply(lambda t: t.month).values.reshape(-1, 1) \n",
    "    #foo['day_of_week'] = df['time1'].apply(lambda t: t.weekday()).values.reshape(-1, 1)\n",
    "    #foo['year_month'] = df['time1'].apply(lambda t: 100 * t.year + t.month).values.reshape(-1, 1) / 1e5\n",
    "    \n",
    "    #foo['day_of_week'] = df['time1'].apply(lambda t: t.weekday()).values.reshape(-1, 1)\n",
    "    #foo['scaled_day_of_week'] = MinMaxScaler().fit_transform(foo['day_of_week'].values.reshape(-1, 1))\n",
    "    \n",
    "    # Added in 28\n",
    "    #foo['lt_48_secs'] = (foo['seconds'] < 48)\n",
    "    #foo['lt_48_secs'] = foo['lt_48_secs'].astype('float64')\n",
    "\n",
    "    #foo['gte_48_secs'] = (foo['seconds'] >= 48)\n",
    "    #foo['gte_48_secs'] = foo['gte_48_secs'].astype('float64')\n",
    "    \n",
    "    foo = foo.drop(columns=['min_seconds', 'max_seconds', 'seconds',])\n",
    "    #foo = foo.drop(columns=['min_seconds', 'max_seconds', 'seconds', 'scaled_session_duration_seconds'])\n",
    "    \n",
    "    X = hstack([X_sparse, foo])\n",
    "    return X\n",
    "    #return foo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_new3 = add_session_length_feature(train_df, X_train_new2)\n",
    "X_test_new3 = add_session_length_feature(test_df, X_test_new2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_scores = cross_val_score(logit2, X_train_new3, y_train, cv=time_split, \n",
    "                            scoring='roc_auc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.90804806, 0.87704985, 0.86902752, 0.98145326, 0.93079662,\n",
       "        0.96936264, 0.94564689, 0.95930799, 0.90358909, 0.97097777]),\n",
       " 0.038285704307677526,\n",
       " 0.9315259678967515)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_scores, cv_scores.std(), cv_scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[38132, 41285, 80, 38133, 15317, 656, 16672, 874, 837, 6581, 38134, 24557, 77, 1307, 12597, 76, 14774, 1345, 75, 74, 240, 876, 16606, 873, 28062, 2329, 1056, 676, 2574, 13347]\n"
     ]
    }
   ],
   "source": [
    "facebook_ids = []\n",
    "youtube_ids = []\n",
    "google_video_ids = []\n",
    "nih_ids = []\n",
    "gmail_ids = []\n",
    "annotathon_ids = []\n",
    "bing_ids = []\n",
    "phylogeny_ids = []\n",
    "oracle_ids = []\n",
    "youwatch_ids = []\n",
    "digi_ids = []\n",
    "verisign_ids = []\n",
    "\n",
    "for key in list(site_dict.keys()):\n",
    "    if 'facebook' in key:\n",
    "        facebook_ids.append(site_dict[key])\n",
    "    if 'youtube' in key or 'ytimg' in key:\n",
    "        youtube_ids.append(site_dict[key])\n",
    "    if 'googlevideo.com' in key:\n",
    "        google_video_ids.append(site_dict[key])\n",
    "    if 'nih.gov' in key:\n",
    "        nih_ids.append(site_dict[key])\n",
    "    if 'mail.google.com' in key:\n",
    "        gmail_ids.append(site_dict[key])\n",
    "    if 'annotathon.org' in key:\n",
    "        annotathon_ids.append(site_dict[key])\n",
    "    if 'bing.com' == key:\n",
    "        bing_ids.append(site_dict[key])\n",
    "    if 'phylogeny.fr' in key:\n",
    "        phylogeny_ids.append(site_dict[key])\n",
    "    if 'javadl-esd-secure.oracle.com' in key or 'download.jboss.org' in key:\n",
    "        oracle_ids.append(site_dict[key])\n",
    "    if 'plus.google.com' in key in key:\n",
    "        youwatch_ids.append(site_dict[key])\n",
    "    if 'safebrowsing-cache.google.com' in key or 'safebrowsing.clients.google.com' in key:\n",
    "        digi_ids.append(site_dict[key])\n",
    "    if 'ocsp.verisign.com' in key or 'gtssl-ocsp.geotrust.com' in key:\n",
    "        verisign_ids.append(site_dict[key])\n",
    "        \n",
    "print(youtube_ids)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_site(x, l):\n",
    "    if x in l:\n",
    "      return 4\n",
    "    return 0\n",
    "\n",
    "def is_long_session(x):\n",
    "    if x < 3:\n",
    "        return 0\n",
    "    elif x < 5:\n",
    "        return 1\n",
    "    elif x < 10:\n",
    "        return 2\n",
    "    elif x < 30:\n",
    "        return 3\n",
    "    elif x < 40:\n",
    "        return 4\n",
    "    return 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_social_network_feature(df, X_sparse):    \n",
    "    foo = pd.DataFrame(index=df.index)\n",
    "    \n",
    "    foo['time1'] = df[['time1']].apply(pd.to_datetime)\n",
    "    # TODO should not fillna, instead find the last date?\n",
    "    foo['time10'] = df[['time10']].fillna('2014-02-20 10:02:45').apply(pd.to_datetime)\n",
    "    \n",
    "    foo['verisign_ids start'] = df['site1'].apply(lambda x: is_site(x, verisign_ids))\n",
    "    foo['verisign_ids start2'] = df['site2'].apply(lambda x: is_site(x, verisign_ids))\n",
    "    foo['verisign_ids start3'] = df['site3'].apply(lambda x: is_site(x, verisign_ids))\n",
    "    foo['verisign_ids start4'] = df['site4'].apply(lambda x: is_site(x, verisign_ids))\n",
    "    foo['verisign_ids start5'] = df['site5'].apply(lambda x: is_site(x, verisign_ids))\n",
    "    foo['verisign_ids start6'] = df['site6'].apply(lambda x: is_site(x, verisign_ids))\n",
    "    foo['verisign_ids start7'] = df['site7'].apply(lambda x: is_site(x, verisign_ids))\n",
    "    foo['verisign_ids start8'] = df['site8'].apply(lambda x: is_site(x, verisign_ids))\n",
    "    foo['verisign_ids start9'] = df['site9'].apply(lambda x: is_site(x, verisign_ids))\n",
    "    foo['verisign_ids start10'] = df['site10'].apply(lambda x: is_site(x, verisign_ids))\n",
    "    \n",
    "    foo['digi start'] = df['site1'].apply(lambda x: is_site(x, digi_ids))\n",
    "    foo['digi start2'] = df['site2'].apply(lambda x: is_site(x, digi_ids))\n",
    "    foo['digi start3'] = df['site3'].apply(lambda x: is_site(x, digi_ids))\n",
    "    foo['digi start4'] = df['site4'].apply(lambda x: is_site(x, digi_ids))\n",
    "    foo['digi start5'] = df['site5'].apply(lambda x: is_site(x, digi_ids))\n",
    "    foo['digi start6'] = df['site6'].apply(lambda x: is_site(x, digi_ids))\n",
    "    foo['digi start7'] = df['site7'].apply(lambda x: is_site(x, digi_ids))\n",
    "    foo['digi start8'] = df['site8'].apply(lambda x: is_site(x, digi_ids))\n",
    "    foo['digi start9'] = df['site9'].apply(lambda x: is_site(x, digi_ids))\n",
    "    foo['digi start10'] = df['site10'].apply(lambda x: is_site(x, digi_ids))\n",
    "    \n",
    "    \n",
    "    foo['youwatch start'] = df['site1'].apply(lambda x: is_site(x, youwatch_ids))\n",
    "    foo['youwatch start2'] = df['site2'].apply(lambda x: is_site(x, youwatch_ids))\n",
    "    foo['youwatch start3'] = df['site3'].apply(lambda x: is_site(x, youwatch_ids))\n",
    "    foo['youwatch start4'] = df['site4'].apply(lambda x: is_site(x, youwatch_ids))\n",
    "    foo['youwatch start5'] = df['site5'].apply(lambda x: is_site(x, youwatch_ids))\n",
    "    foo['youwatch star6'] = df['site6'].apply(lambda x: is_site(x, youwatch_ids))\n",
    "    foo['youwatch start7'] = df['site7'].apply(lambda x: is_site(x, youwatch_ids))\n",
    "    foo['youwatch start8'] = df['site8'].apply(lambda x: is_site(x, youwatch_ids))\n",
    "    foo['youwatch start9'] = df['site9'].apply(lambda x: is_site(x, youwatch_ids))\n",
    "    foo['youwatch start10'] = df['site10'].apply(lambda x: is_site(x, youwatch_ids))\n",
    "\n",
    "    foo['oracle start1'] = df['site1'].apply(lambda x: is_site(x, oracle_ids))\n",
    "    foo['oracle start2'] = df['site2'].apply(lambda x: is_site(x, oracle_ids))\n",
    "    foo['oracle start3'] = df['site3'].apply(lambda x: is_site(x, oracle_ids))\n",
    "    foo['oracle start4'] = df['site4'].apply(lambda x: is_site(x, oracle_ids))\n",
    "    foo['oracle start5'] = df['site5'].apply(lambda x: is_site(x, oracle_ids))\n",
    "    foo['oracle start6'] = df['site1'].apply(lambda x: is_site(x, oracle_ids))\n",
    "    foo['oracle start7'] = df['site2'].apply(lambda x: is_site(x, oracle_ids))\n",
    "    foo['oracle start8'] = df['site3'].apply(lambda x: is_site(x, oracle_ids))\n",
    "    foo['oracle start9'] = df['site4'].apply(lambda x: is_site(x, oracle_ids))\n",
    "    foo['oracle start10'] = df['site5'].apply(lambda x: is_site(x, oracle_ids))\n",
    "\n",
    "    foo['gmail start'] = df['site1'].apply(lambda x: is_site(x, gmail_ids))\n",
    "    foo['gmail start2'] = df['site2'].apply(lambda x: is_site(x, gmail_ids))\n",
    "    foo['gmail start3'] = df['site3'].apply(lambda x: is_site(x, gmail_ids))\n",
    "    foo['gmail start4'] = df['site4'].apply(lambda x: is_site(x, gmail_ids))\n",
    "    foo['gmail start5'] = df['site5'].apply(lambda x: is_site(x, gmail_ids))\n",
    "    foo['gmail start6'] = df['site6'].apply(lambda x: is_site(x, gmail_ids))\n",
    "    foo['gmail start7'] = df['site7'].apply(lambda x: is_site(x, gmail_ids))\n",
    "    foo['gmail start8'] = df['site8'].apply(lambda x: is_site(x, gmail_ids))\n",
    "    foo['gmail start9'] = df['site9'].apply(lambda x: is_site(x, gmail_ids))\n",
    "    foo['gmail start10'] = df['site10'].apply(lambda x: is_site(x, gmail_ids))\n",
    "    \n",
    "    foo['bing start1'] = df['site1'].apply(lambda x: is_site(x, bing_ids))\n",
    "    foo['bing start2'] = df['site2'].apply(lambda x: is_site(x, bing_ids))\n",
    "    foo['bing start3'] = df['site3'].apply(lambda x: is_site(x, bing_ids))\n",
    "    foo['bing start4'] = df['site4'].apply(lambda x: is_site(x, bing_ids))\n",
    "    foo['bing start5'] = df['site5'].apply(lambda x: is_site(x, bing_ids))\n",
    "    foo['bing start6'] = df['site6'].apply(lambda x: is_site(x, bing_ids))\n",
    "    foo['bing start7'] = df['site7'].apply(lambda x: is_site(x, bing_ids))\n",
    "    foo['bing start8'] = df['site8'].apply(lambda x: is_site(x, bing_ids))\n",
    "    foo['bing start9'] = df['site9'].apply(lambda x: is_site(x, bing_ids))\n",
    "    foo['bing start10'] = df['site10'].apply(lambda x: is_site(x, bing_ids))\n",
    "    \n",
    "    foo['phylogeny start'] = df['site1'].apply(lambda x: is_site(x, phylogeny_ids))\n",
    "    foo['phylogeny start2'] = df['site2'].apply(lambda x: is_site(x, phylogeny_ids))\n",
    "    foo['phylogeny start3'] = df['site3'].apply(lambda x: is_site(x, phylogeny_ids))\n",
    "    foo['phylogeny start4'] = df['site4'].apply(lambda x: is_site(x, phylogeny_ids))\n",
    "    foo['phylogeny start5'] = df['site5'].apply(lambda x: is_site(x, phylogeny_ids))\n",
    "    foo['phylogeny start6'] = df['site6'].apply(lambda x: is_site(x, phylogeny_ids))\n",
    "    foo['phylogeny start7'] = df['site7'].apply(lambda x: is_site(x, phylogeny_ids))\n",
    "    foo['phylogeny start8'] = df['site8'].apply(lambda x: is_site(x, phylogeny_ids))\n",
    "    foo['phylogeny start9'] = df['site9'].apply(lambda x: is_site(x, phylogeny_ids))\n",
    "    foo['phylogeny start10'] = df['site10'].apply(lambda x: is_site(x, phylogeny_ids))\n",
    "\n",
    "    foo['nih start'] = df['site1'].apply(lambda x: is_site(x, nih_ids))\n",
    "    foo['nih start2'] = df['site2'].apply(lambda x: is_site(x, nih_ids))\n",
    "    foo['nih start3'] = df['site3'].apply(lambda x: is_site(x, nih_ids))\n",
    "    foo['nih start4'] = df['site4'].apply(lambda x: is_site(x, nih_ids))\n",
    "    foo['nih start5'] = df['site5'].apply(lambda x: is_site(x, nih_ids))\n",
    "    foo['nih start6'] = df['site6'].apply(lambda x: is_site(x, nih_ids))\n",
    "    foo['nih start7'] = df['site7'].apply(lambda x: is_site(x, nih_ids))\n",
    "    foo['nih start8'] = df['site8'].apply(lambda x: is_site(x, nih_ids))\n",
    "    foo['nih start9'] = df['site9'].apply(lambda x: is_site(x, nih_ids))\n",
    "    foo['nih start10'] = df['site10'].apply(lambda x: is_site(x, nih_ids))\n",
    "\n",
    "    foo['annotathon start'] = df['site1'].apply(lambda x: is_site(x, annotathon_ids))\n",
    "    foo['annotathon start2'] = df['site2'].apply(lambda x: is_site(x, annotathon_ids))\n",
    "    foo['annotathon start3'] = df['site3'].apply(lambda x: is_site(x, annotathon_ids))\n",
    "    foo['annotathon start4'] = df['site4'].apply(lambda x: is_site(x, annotathon_ids))\n",
    "    foo['annotathon start5'] = df['site5'].apply(lambda x: is_site(x, annotathon_ids))\n",
    "    foo['annotathon start6'] = df['site6'].apply(lambda x: is_site(x, annotathon_ids))\n",
    "    foo['annotathon start7'] = df['site7'].apply(lambda x: is_site(x, annotathon_ids))\n",
    "    foo['annotathon start8'] = df['site8'].apply(lambda x: is_site(x, annotathon_ids))\n",
    "    foo['annotathon start9'] = df['site9'].apply(lambda x: is_site(x, annotathon_ids))\n",
    "    foo['annotathon start10'] = df['site10'].apply(lambda x: is_site(x, annotathon_ids))\n",
    "    \n",
    "\n",
    "    \n",
    "    foo['start day'] = foo['time1'].apply(pd.datetime.weekday)\n",
    "    foo['end day'] = foo['time10'].apply(pd.datetime.weekday)\n",
    "    \n",
    "    foo = foo.drop(columns=['time1', 'time10'])\n",
    "    \n",
    "    X = hstack([X_sparse, foo])\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_new4 = add_social_network_feature(train_df, X_train_new3)\n",
    "X_test_new4 = add_social_network_feature(test_df, X_test_new3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_scores = cross_val_score(logit2, X_train_new4, y_train, cv=time_split, \n",
    "                            scoring='roc_auc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.91510007, 0.88829327, 0.8808899 , 0.98178108, 0.93578611,\n",
       "        0.97177937, 0.95512493, 0.96186566, 0.92008902, 0.97321655]),\n",
       " 0.03426625001152704,\n",
       " 0.9383925963246806)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_scores, cv_scores.std(), cv_scores.mean() # 0.934 0.9355990452519014 0.9383925"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'C': np.logspace(-2, 2, 10)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_grid_searcher = GridSearchCV(estimator=logit2, param_grid=params,\n",
    "                                  scoring='roc_auc', cv=time_split, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 10 candidates, totalling 100 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:  6.1min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=TimeSeriesSplit(max_train_size=None, n_splits=10),\n",
       "             error_score='raise-deprecating',\n",
       "             estimator=LogisticRegression(C=1, class_weight=None, dual=False,\n",
       "                                          fit_intercept=True,\n",
       "                                          intercept_scaling=1, l1_ratio=None,\n",
       "                                          max_iter=100, multi_class='warn',\n",
       "                                          n_jobs=None, penalty='l2',\n",
       "                                          random_state=17, solver='liblinear',\n",
       "                                          tol=0.0001, verbose=0,\n",
       "                                          warm_start=False),\n",
       "             iid='warn', n_jobs=None,\n",
       "             param_grid={'C': array([1.00000000e-02, 2.78255940e-02, 7.74263683e-02, 2.15443469e-01,\n",
       "       5.99484250e-01, 1.66810054e+00, 4.64158883e+00, 1.29154967e+01,\n",
       "       3.59381366e+01, 1.00000000e+02])},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring='roc_auc', verbose=1)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit_grid_searcher.fit(X_train_new4, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.938744616706427, {'C': 1.6681005372000592})"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit_grid_searcher.best_score_, logit_grid_searcher.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_test_pred = logit_grid_searcher.best_estimator_.predict_proba(X_test_new4)[:, 1]\n",
    "write_to_submission_file(logit_test_pred, 'submissions33331.csv') #0.95871"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
